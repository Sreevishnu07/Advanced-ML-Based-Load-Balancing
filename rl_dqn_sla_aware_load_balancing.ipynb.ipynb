{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wf5KrEb6vrkR"
      },
      "source": [
        "# Welcome to Colab!"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vcUoEtljIa9f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kFWUZCimIav7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "09lIli-iIaee"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "6ya5oYW6IaP5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "wUjRb7W-IZ-8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from google.colab import files\n",
        "import os\n",
        "\n",
        "CSV_PATH = \"/content/load_balancing_playground.csv\"\n",
        "\n",
        "if not os.path.exists(CSV_PATH):\n",
        "    print(\"Upload load_balancing_playground.csv\")\n",
        "    uploaded = files.upload()\n",
        "    uploaded_filename = list(uploaded.keys())[0]\n",
        "    os.rename(uploaded_filename, CSV_PATH)\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "class LoadBalancingEnv:\n",
        "    \"\"\"\n",
        "    MDP Environment for RL-based Load Balancing\n",
        "    Each step = assign one task to one CPU\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        csv_path,\n",
        "        num_cpus=4,\n",
        "        lambda_imbalance=0.5,\n",
        "        mu_sla=2.0,\n",
        "        episode_length=1000,\n",
        "    ):\n",
        "        self.df = pd.read_csv(csv_path)\n",
        "        self.num_cpus = num_cpus\n",
        "\n",
        "        self.lambda_imbalance = lambda_imbalance\n",
        "        self.mu_sla = mu_sla\n",
        "        self.episode_length = episode_length\n",
        "\n",
        "        self.current_step = 0\n",
        "        self.start_index = 0\n",
        "\n",
        "    def reset(self):\n",
        "        self.start_index = np.random.randint(\n",
        "            0, len(self.df) - self.episode_length\n",
        "        )\n",
        "        self.current_step = 0\n",
        "        return self._get_state()\n",
        "\n",
        "    def step(self, action):\n",
        "        \"\"\"\n",
        "        action: integer in [0, num_cpus - 1]\n",
        "        \"\"\"\n",
        "        row = self.df.iloc[self.start_index + self.current_step]\n",
        "\n",
        "        completion_time = (\n",
        "            row[f\"cpu_{action}_wait_time\"]\n",
        "            + row[\"task_size\"] / row[f\"cpu_{action}_speed\"]\n",
        "        )\n",
        "\n",
        "        load_variance = row[\"load_variance\"]\n",
        "        sla_violation = row[\"sla_violation\"]\n",
        "\n",
        "        reward = (\n",
        "            -completion_time\n",
        "            - self.lambda_imbalance * load_variance\n",
        "            - self.mu_sla * sla_violation\n",
        "        )\n",
        "\n",
        "        self.current_step += 1\n",
        "        done = self.current_step >= self.episode_length\n",
        "\n",
        "        next_state = self._get_state() if not done else None\n",
        "\n",
        "        info = {\n",
        "            \"completion_time\": completion_time,\n",
        "            \"sla_violation\": sla_violation,\n",
        "            \"oracle_cpu\": row[\"chosen_cpu_oracle\"],\n",
        "        }\n",
        "\n",
        "        return next_state, reward, done, info\n",
        "\n",
        "    def _get_state(self):\n",
        "        row = self.df.iloc[self.start_index + self.current_step]\n",
        "\n",
        "        state = [\n",
        "            row[\"task_size\"],\n",
        "            row[\"priority\"],\n",
        "            row[\"system_load_avg\"],\n",
        "            row[\"load_variance\"],\n",
        "        ]\n",
        "\n",
        "        for i in range(self.num_cpus):\n",
        "            state.extend([\n",
        "                row[f\"cpu_{i}_queue_len\"],\n",
        "                row[f\"cpu_{i}_load\"],\n",
        "                row[f\"cpu_{i}_wait_time\"],\n",
        "                row[f\"cpu_{i}_speed\"],\n",
        "            ])\n",
        "\n",
        "        return np.array(state, dtype=np.float32)\n",
        "\n",
        "    def action_space(self):\n",
        "        return self.num_cpus\n",
        "\n",
        "    def state_dim(self):\n",
        "        return len(self._get_state())\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "env = LoadBalancingEnv(csv_path=CSV_PATH)\n",
        "\n",
        "state = env.reset()\n",
        "print(\"State dimension:\", env.state_dim())\n",
        "print(\"Action space:\", env.action_space())\n",
        "\n",
        "total_reward = 0\n",
        "\n",
        "for step in range(5):\n",
        "    action = np.random.randint(0, env.action_space())\n",
        "    next_state, reward, done, info = env.step(action)\n",
        "    total_reward += reward\n",
        "\n",
        "    print(\n",
        "        f\"Step {step} | Action: CPU-{action} | \"\n",
        "        f\"Reward: {reward:.3f} | \"\n",
        "        f\"Oracle CPU: {info['oracle_cpu']}\"\n",
        "    )\n",
        "\n",
        "print(\"Total reward (sample):\", total_reward)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 229
        },
        "id": "yywcodhGAge5",
        "outputId": "66429364-c81e-4131-8610-ffba3f7594b0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Upload load_balancing_playground.csv\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-7dc2e5be-80c1-4e33-9c1b-ef6c2c0f8543\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-7dc2e5be-80c1-4e33-9c1b-ef6c2c0f8543\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving load_balancing_playground.csv to load_balancing_playground.csv\n",
            "State dimension: 20\n",
            "Action space: 4\n",
            "Step 0 | Action: CPU-2 | Reward: -65.908 | Oracle CPU: 0\n",
            "Step 1 | Action: CPU-2 | Reward: -21.430 | Oracle CPU: 1\n",
            "Step 2 | Action: CPU-2 | Reward: -41.351 | Oracle CPU: 2\n",
            "Step 3 | Action: CPU-3 | Reward: -50.906 | Oracle CPU: 2\n",
            "Step 4 | Action: CPU-0 | Reward: -8.125 | Oracle CPU: 2\n",
            "Total reward (sample): -187.71853351143304\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def round_robin_policy(step, num_cpus):\n",
        "    return step % num_cpus\n",
        "\n",
        "def lwr_policy(state, num_cpus):\n",
        "    \"\"\"\n",
        "    Choose CPU with minimum estimated work remaining\n",
        "    \"\"\"\n",
        "    idx = 4\n",
        "    scores = []\n",
        "    for i in range(num_cpus):\n",
        "        queue_len = state[idx]\n",
        "        load = state[idx + 1]\n",
        "        wait = state[idx + 2]\n",
        "        speed = state[idx + 3]\n",
        "        scores.append(wait / speed)\n",
        "        idx += 4\n",
        "    return int(np.argmin(scores))\n",
        "\n",
        "def oracle_policy(info):\n",
        "    return info[\"oracle_cpu\"]\n",
        "\n",
        "\n",
        "def evaluate_policy(env, policy_name, episodes=5):\n",
        "    total_rewards = []\n",
        "    oracle_matches = []\n",
        "    sla_violations = []\n",
        "\n",
        "    for ep in range(episodes):\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "        step = 0\n",
        "\n",
        "        ep_reward = 0\n",
        "        ep_oracle_match = 0\n",
        "        ep_sla = 0\n",
        "        total_steps = 0\n",
        "\n",
        "        while not done:\n",
        "            if policy_name == \"RR\":\n",
        "                action = round_robin_policy(step, env.action_space())\n",
        "            elif policy_name == \"LWR\":\n",
        "                action = lwr_policy(state, env.action_space())\n",
        "            else:\n",
        "                raise ValueError(\"Unknown policy\")\n",
        "\n",
        "            next_state, reward, done, info = env.step(action)\n",
        "\n",
        "            ep_reward += reward\n",
        "            ep_oracle_match += int(action == info[\"oracle_cpu\"])\n",
        "            ep_sla += info[\"sla_violation\"]\n",
        "            total_steps += 1\n",
        "\n",
        "            state = next_state\n",
        "            step += 1\n",
        "\n",
        "        total_rewards.append(ep_reward / total_steps)\n",
        "        oracle_matches.append(ep_oracle_match / total_steps)\n",
        "        sla_violations.append(ep_sla / total_steps)\n",
        "\n",
        "    print(f\"\\n=== {policy_name} POLICY RESULTS ===\")\n",
        "    print(f\"Avg Reward      : {np.mean(total_rewards):.3f}\")\n",
        "    print(f\"Oracle Accuracy : {np.mean(oracle_matches)*100:.2f}%\")\n",
        "    print(f\"SLA Violation % : {np.mean(sla_violations)*100:.2f}%\")\n",
        "\n",
        "\n",
        "env = LoadBalancingEnv(\n",
        "    csv_path=CSV_PATH,\n",
        "    episode_length=1000\n",
        ")\n",
        "\n",
        "evaluate_policy(env, \"RR\")\n",
        "evaluate_policy(env, \"LWR\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-LMm8-EWAqA-",
        "outputId": "d5780e1c-b909-442e-8fc2-6ef895ca4296"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== RR POLICY RESULTS ===\n",
            "Avg Reward      : -38.513\n",
            "Oracle Accuracy : 24.78%\n",
            "SLA Violation % : 33.18%\n",
            "\n",
            "=== LWR POLICY RESULTS ===\n",
            "Avg Reward      : -14.916\n",
            "Oracle Accuracy : 86.00%\n",
            "SLA Violation % : 33.06%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import random\n",
        "from collections import deque\n",
        "import numpy as np\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "class DQN(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(state_dim, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, action_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "\n",
        "class ReplayBuffer:\n",
        "    def __init__(self, capacity=50000):\n",
        "        self.buffer = deque(maxlen=capacity)\n",
        "\n",
        "    def push(self, state, action, reward, next_state, done):\n",
        "        self.buffer.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        batch = random.sample(self.buffer, batch_size)\n",
        "        states, actions, rewards, next_states, dones = zip(*batch)\n",
        "        return (\n",
        "            torch.tensor(states, dtype=torch.float32).to(device),\n",
        "            torch.tensor(actions).to(device),\n",
        "            torch.tensor(rewards, dtype=torch.float32).to(device),\n",
        "            torch.tensor(next_states, dtype=torch.float32).to(device),\n",
        "            torch.tensor(dones, dtype=torch.float32).to(device),\n",
        "        )\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)\n",
        "\n",
        "\n",
        "state_dim = env.state_dim()\n",
        "action_dim = env.action_space()\n",
        "\n",
        "policy_net = DQN(state_dim, action_dim).to(device)\n",
        "target_net = DQN(state_dim, action_dim).to(device)\n",
        "target_net.load_state_dict(policy_net.state_dict())\n",
        "target_net.eval()\n",
        "\n",
        "optimizer = optim.Adam(policy_net.parameters(), lr=1e-3)\n",
        "buffer = ReplayBuffer()\n",
        "\n",
        "gamma = 0.99\n",
        "batch_size = 64\n",
        "epsilon = 1.0\n",
        "epsilon_min = 0.05\n",
        "epsilon_decay = 0.995\n",
        "target_update = 200\n",
        "\n",
        "episodes = 50\n",
        "max_steps = 1000\n",
        "\n",
        "for ep in range(episodes):\n",
        "    state = env.reset()\n",
        "    total_reward = 0\n",
        "    sla_violations = 0\n",
        "    oracle_matches = 0\n",
        "\n",
        "    for step in range(max_steps):\n",
        "\n",
        "        if random.random() < epsilon:\n",
        "            action = random.randint(0, action_dim - 1)\n",
        "        else:\n",
        "            with torch.no_grad():\n",
        "                q_vals = policy_net(\n",
        "                    torch.tensor(state, dtype=torch.float32).to(device)\n",
        "                )\n",
        "                action = int(torch.argmax(q_vals).item())\n",
        "\n",
        "        next_state, reward, done, info = env.step(action)\n",
        "\n",
        "        buffer.push(state, action, reward, next_state, done)\n",
        "\n",
        "        total_reward += reward\n",
        "        sla_violations += info[\"sla_violation\"]\n",
        "        oracle_matches += int(action == info[\"oracle_cpu\"])\n",
        "\n",
        "        state = next_state\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "        # Learn\n",
        "        if len(buffer) >= batch_size:\n",
        "            states, actions, rewards, next_states, dones = buffer.sample(batch_size)\n",
        "\n",
        "            q_values = policy_net(states).gather(1, actions.unsqueeze(1)).squeeze()\n",
        "            next_q = target_net(next_states).max(1)[0]\n",
        "            target = rewards + gamma * next_q * (1 - dones)\n",
        "\n",
        "            loss = nn.MSELoss()(q_values, target.detach())\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        if step % target_update == 0:\n",
        "            target_net.load_state_dict(policy_net.state_dict())\n",
        "\n",
        "    epsilon = max(epsilon * epsilon_decay, epsilon_min)\n",
        "\n",
        "    print(\n",
        "        f\"Episode {ep+1:02d} | \"\n",
        "        f\"Avg Reward: {total_reward/max_steps:.3f} | \"\n",
        "        f\"Oracle Acc: {oracle_matches/max_steps*100:.2f}% | \"\n",
        "        f\"SLA %: {sla_violations/max_steps*100:.2f}% | \"\n",
        "        f\"Epsilon: {epsilon:.3f}\"\n",
        "    )\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 404
        },
        "id": "Gmdt0UYyCpnp",
        "outputId": "2d15a02e-b76b-48a7-8972-1007402a74ae"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1743863237.py:46: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)\n",
            "  torch.tensor(states, dtype=torch.float32).to(device),\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 01 | Avg Reward: -39.165 | Oracle Acc: 24.70% | SLA %: 35.70% | Epsilon: 0.995\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "not a sequence",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1743863237.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0;31m# Learn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdones\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m             \u001b[0mq_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolicy_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1743863237.py\u001b[0m in \u001b[0;36msample\u001b[0;34m(self, batch_size)\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdones\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         )\n",
            "\u001b[0;31mTypeError\u001b[0m: not a sequence"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "class LoadBalancingEnv:\n",
        "    ...\n",
        "\n"
      ],
      "metadata": {
        "id": "wQ7JNlcCDFyy"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "...\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rfgoQiOeDJD1",
        "outputId": "3f0e96f9-aa1b-4fdf-dc3e-2c455ab3de38"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Ellipsis"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env = LoadBalancingEnv(CSV_PATH)\n",
        "...\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "id": "H0-Bda1rDLt-",
        "outputId": "2a6b3c41-f5e4-4ab0-962f-f2034ee3271b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "LoadBalancingEnv() takes no arguments",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2997317876.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# DQN training loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLoadBalancingEnv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCSV_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: LoadBalancingEnv() takes no arguments"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from google.colab import files\n",
        "import os\n",
        "\n",
        "CSV_PATH = \"/content/load_balancing_playground.csv\"\n",
        "\n",
        "if not os.path.exists(CSV_PATH):\n",
        "    print(\"Upload load_balancing_playground.csv\")\n",
        "    uploaded = files.upload()\n",
        "    uploaded_filename = list(uploaded.keys())[0]\n",
        "    os.rename(uploaded_filename, CSV_PATH)\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import random\n",
        "from collections import deque\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "\n",
        "class LoadBalancingEnv:\n",
        "    def __init__(\n",
        "        self,\n",
        "        csv_path,\n",
        "        num_cpus=4,\n",
        "        lambda_imbalance=0.5,\n",
        "        mu_sla=2.0,\n",
        "        episode_length=1000,\n",
        "    ):\n",
        "        self.df = pd.read_csv(csv_path)\n",
        "        self.num_cpus = num_cpus\n",
        "        self.lambda_imbalance = lambda_imbalance\n",
        "        self.mu_sla = mu_sla\n",
        "        self.episode_length = episode_length\n",
        "        self.current_step = 0\n",
        "        self.start_index = 0\n",
        "\n",
        "    def reset(self):\n",
        "        self.start_index = np.random.randint(\n",
        "            0, len(self.df) - self.episode_length\n",
        "        )\n",
        "        self.current_step = 0\n",
        "        return self._get_state()\n",
        "\n",
        "    def step(self, action):\n",
        "        row = self.df.iloc[self.start_index + self.current_step]\n",
        "\n",
        "        completion_time = (\n",
        "            row[f\"cpu_{action}_wait_time\"]\n",
        "            + row[\"task_size\"] / row[f\"cpu_{action}_speed\"]\n",
        "        )\n",
        "\n",
        "        reward = (\n",
        "            -completion_time\n",
        "            - self.lambda_imbalance * row[\"load_variance\"]\n",
        "            - self.mu_sla * row[\"sla_violation\"]\n",
        "        )\n",
        "\n",
        "        self.current_step += 1\n",
        "        done = self.current_step >= self.episode_length\n",
        "\n",
        "        next_state = self._get_state() if not done else None\n",
        "\n",
        "        info = {\n",
        "            \"oracle_cpu\": row[\"chosen_cpu_oracle\"],\n",
        "            \"sla_violation\": row[\"sla_violation\"],\n",
        "        }\n",
        "\n",
        "        return next_state, reward, done, info\n",
        "\n",
        "    def _get_state(self):\n",
        "        row = self.df.iloc[self.start_index + self.current_step]\n",
        "        state = [\n",
        "            row[\"task_size\"],\n",
        "            row[\"priority\"],\n",
        "            row[\"system_load_avg\"],\n",
        "            row[\"load_variance\"],\n",
        "        ]\n",
        "        for i in range(self.num_cpus):\n",
        "            state.extend([\n",
        "                row[f\"cpu_{i}_queue_len\"],\n",
        "                row[f\"cpu_{i}_load\"],\n",
        "                row[f\"cpu_{i}_wait_time\"],\n",
        "                row[f\"cpu_{i}_speed\"],\n",
        "            ])\n",
        "        return np.array(state, dtype=np.float32)\n",
        "\n",
        "    def action_space(self):\n",
        "        return self.num_cpus\n",
        "\n",
        "    def state_dim(self):\n",
        "        return len(self._get_state())\n",
        "\n",
        "\n",
        "class DQN(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(state_dim, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, action_dim),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class ReplayBuffer:\n",
        "    def __init__(self, state_dim, capacity=50000):\n",
        "        self.buffer = deque(maxlen=capacity)\n",
        "        self.state_dim = state_dim\n",
        "\n",
        "    def push(self, state, action, reward, next_state, done):\n",
        "        if next_state is None:\n",
        "            next_state = np.zeros(self.state_dim, dtype=np.float32)\n",
        "        self.buffer.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        batch = random.sample(self.buffer, batch_size)\n",
        "        states, actions, rewards, next_states, dones = zip(*batch)\n",
        "        return (\n",
        "            torch.from_numpy(np.array(states)).float().to(device),\n",
        "            torch.tensor(actions).long().to(device),\n",
        "            torch.tensor(rewards).float().to(device),\n",
        "            torch.from_numpy(np.array(next_states)).float().to(device),\n",
        "            torch.tensor(dones).float().to(device),\n",
        "        )\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)\n",
        "\n",
        "\n",
        "env = LoadBalancingEnv(CSV_PATH)\n",
        "state_dim = env.state_dim()\n",
        "action_dim = env.action_space()\n",
        "\n",
        "policy_net = DQN(state_dim, action_dim).to(device)\n",
        "target_net = DQN(state_dim, action_dim).to(device)\n",
        "target_net.load_state_dict(policy_net.state_dict())\n",
        "target_net.eval()\n",
        "\n",
        "optimizer = optim.Adam(policy_net.parameters(), lr=1e-3)\n",
        "buffer = ReplayBuffer(state_dim)\n",
        "\n",
        "gamma = 0.99\n",
        "batch_size = 64\n",
        "epsilon = 1.0\n",
        "epsilon_min = 0.05\n",
        "epsilon_decay = 0.995\n",
        "target_update = 200\n",
        "\n",
        "episodes = 50\n",
        "max_steps = 1000\n",
        "\n",
        "for ep in range(episodes):\n",
        "    state = env.reset()\n",
        "    total_reward = 0\n",
        "    oracle_hits = 0\n",
        "    sla = 0\n",
        "\n",
        "    for step in range(max_steps):\n",
        "        if random.random() < epsilon:\n",
        "            action = random.randint(0, action_dim - 1)\n",
        "        else:\n",
        "            with torch.no_grad():\n",
        "                q_vals = policy_net(\n",
        "                    torch.tensor(state).float().to(device)\n",
        "                )\n",
        "                action = int(torch.argmax(q_vals).item())\n",
        "\n",
        "        next_state, reward, done, info = env.step(action)\n",
        "\n",
        "        buffer.push(state, action, reward, next_state, done)\n",
        "\n",
        "        total_reward += reward\n",
        "        oracle_hits += int(action == info[\"oracle_cpu\"])\n",
        "        sla += info[\"sla_violation\"]\n",
        "\n",
        "        state = next_state\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "        if len(buffer) >= batch_size:\n",
        "            states, actions, rewards, next_states, dones = buffer.sample(batch_size)\n",
        "\n",
        "            q = policy_net(states).gather(1, actions.unsqueeze(1)).squeeze()\n",
        "            next_q = target_net(next_states).max(1)[0]\n",
        "            target = rewards + gamma * next_q * (1 - dones)\n",
        "\n",
        "            loss = nn.MSELoss()(q, target.detach())\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        if step % target_update == 0:\n",
        "            target_net.load_state_dict(policy_net.state_dict())\n",
        "\n",
        "    epsilon = max(epsilon * epsilon_decay, epsilon_min)\n",
        "\n",
        "    print(\n",
        "        f\"Episode {ep+1:02d} | \"\n",
        "        f\"Avg Reward: {total_reward/max_steps:.3f} | \"\n",
        "        f\"Oracle Acc: {oracle_hits/max_steps*100:.2f}% | \"\n",
        "        f\"SLA %: {sla/max_steps*100:.2f}% | \"\n",
        "        f\"Epsilon: {epsilon:.3f}\"\n",
        "    )\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IgSeiDW5DmWm",
        "outputId": "56dc78a8-2493-4049-8cb3-fafdf213dae0"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 01 | Avg Reward: -39.366 | Oracle Acc: 24.40% | SLA %: 35.30% | Epsilon: 0.995\n",
            "Episode 02 | Avg Reward: -37.686 | Oracle Acc: 24.20% | SLA %: 31.30% | Epsilon: 0.990\n",
            "Episode 03 | Avg Reward: -37.723 | Oracle Acc: 25.80% | SLA %: 33.30% | Epsilon: 0.985\n",
            "Episode 04 | Avg Reward: -38.565 | Oracle Acc: 26.40% | SLA %: 33.30% | Epsilon: 0.980\n",
            "Episode 05 | Avg Reward: -39.722 | Oracle Acc: 27.00% | SLA %: 33.10% | Epsilon: 0.975\n",
            "Episode 06 | Avg Reward: -38.485 | Oracle Acc: 26.40% | SLA %: 35.90% | Epsilon: 0.970\n",
            "Episode 07 | Avg Reward: -36.832 | Oracle Acc: 28.40% | SLA %: 33.10% | Epsilon: 0.966\n",
            "Episode 08 | Avg Reward: -37.731 | Oracle Acc: 26.20% | SLA %: 31.80% | Epsilon: 0.961\n",
            "Episode 09 | Avg Reward: -37.725 | Oracle Acc: 25.90% | SLA %: 34.90% | Epsilon: 0.956\n",
            "Episode 10 | Avg Reward: -34.406 | Oracle Acc: 31.50% | SLA %: 32.70% | Epsilon: 0.951\n",
            "Episode 11 | Avg Reward: -39.081 | Oracle Acc: 26.00% | SLA %: 33.20% | Epsilon: 0.946\n",
            "Episode 12 | Avg Reward: -37.081 | Oracle Acc: 27.70% | SLA %: 32.00% | Epsilon: 0.942\n",
            "Episode 13 | Avg Reward: -36.671 | Oracle Acc: 25.50% | SLA %: 32.70% | Epsilon: 0.937\n",
            "Episode 14 | Avg Reward: -38.376 | Oracle Acc: 27.20% | SLA %: 32.80% | Epsilon: 0.932\n",
            "Episode 15 | Avg Reward: -36.947 | Oracle Acc: 28.60% | SLA %: 33.70% | Epsilon: 0.928\n",
            "Episode 16 | Avg Reward: -35.191 | Oracle Acc: 28.60% | SLA %: 32.30% | Epsilon: 0.923\n",
            "Episode 17 | Avg Reward: -36.948 | Oracle Acc: 28.60% | SLA %: 33.90% | Epsilon: 0.918\n",
            "Episode 18 | Avg Reward: -36.293 | Oracle Acc: 32.20% | SLA %: 35.00% | Epsilon: 0.914\n",
            "Episode 19 | Avg Reward: -37.325 | Oracle Acc: 29.00% | SLA %: 33.10% | Epsilon: 0.909\n",
            "Episode 20 | Avg Reward: -35.325 | Oracle Acc: 31.20% | SLA %: 33.00% | Epsilon: 0.905\n",
            "Episode 21 | Avg Reward: -36.148 | Oracle Acc: 31.70% | SLA %: 32.80% | Epsilon: 0.900\n",
            "Episode 22 | Avg Reward: -35.653 | Oracle Acc: 33.30% | SLA %: 36.60% | Epsilon: 0.896\n",
            "Episode 23 | Avg Reward: -36.318 | Oracle Acc: 30.60% | SLA %: 33.40% | Epsilon: 0.891\n",
            "Episode 24 | Avg Reward: -36.336 | Oracle Acc: 30.30% | SLA %: 35.10% | Epsilon: 0.887\n",
            "Episode 25 | Avg Reward: -35.350 | Oracle Acc: 30.40% | SLA %: 31.70% | Epsilon: 0.882\n",
            "Episode 26 | Avg Reward: -37.108 | Oracle Acc: 29.30% | SLA %: 35.70% | Epsilon: 0.878\n",
            "Episode 27 | Avg Reward: -35.355 | Oracle Acc: 33.30% | SLA %: 35.70% | Epsilon: 0.873\n",
            "Episode 28 | Avg Reward: -38.666 | Oracle Acc: 28.20% | SLA %: 35.40% | Epsilon: 0.869\n",
            "Episode 29 | Avg Reward: -34.379 | Oracle Acc: 34.80% | SLA %: 33.50% | Epsilon: 0.865\n",
            "Episode 30 | Avg Reward: -34.143 | Oracle Acc: 30.90% | SLA %: 31.60% | Epsilon: 0.860\n",
            "Episode 31 | Avg Reward: -36.944 | Oracle Acc: 30.80% | SLA %: 32.20% | Epsilon: 0.856\n",
            "Episode 32 | Avg Reward: -33.689 | Oracle Acc: 33.10% | SLA %: 32.00% | Epsilon: 0.852\n",
            "Episode 33 | Avg Reward: -34.427 | Oracle Acc: 33.60% | SLA %: 34.10% | Epsilon: 0.848\n",
            "Episode 34 | Avg Reward: -36.810 | Oracle Acc: 33.10% | SLA %: 35.00% | Epsilon: 0.843\n",
            "Episode 35 | Avg Reward: -34.397 | Oracle Acc: 32.10% | SLA %: 30.10% | Epsilon: 0.839\n",
            "Episode 36 | Avg Reward: -33.229 | Oracle Acc: 35.40% | SLA %: 33.40% | Epsilon: 0.835\n",
            "Episode 37 | Avg Reward: -34.144 | Oracle Acc: 31.80% | SLA %: 31.40% | Epsilon: 0.831\n",
            "Episode 38 | Avg Reward: -32.697 | Oracle Acc: 34.70% | SLA %: 32.70% | Epsilon: 0.827\n",
            "Episode 39 | Avg Reward: -35.129 | Oracle Acc: 36.40% | SLA %: 33.90% | Epsilon: 0.822\n",
            "Episode 40 | Avg Reward: -32.127 | Oracle Acc: 38.10% | SLA %: 32.70% | Epsilon: 0.818\n",
            "Episode 41 | Avg Reward: -34.214 | Oracle Acc: 35.00% | SLA %: 34.10% | Epsilon: 0.814\n",
            "Episode 42 | Avg Reward: -34.460 | Oracle Acc: 35.00% | SLA %: 35.30% | Epsilon: 0.810\n",
            "Episode 43 | Avg Reward: -35.221 | Oracle Acc: 36.00% | SLA %: 33.20% | Epsilon: 0.806\n",
            "Episode 44 | Avg Reward: -34.014 | Oracle Acc: 34.80% | SLA %: 32.20% | Epsilon: 0.802\n",
            "Episode 45 | Avg Reward: -32.361 | Oracle Acc: 38.00% | SLA %: 33.10% | Epsilon: 0.798\n",
            "Episode 46 | Avg Reward: -33.400 | Oracle Acc: 38.00% | SLA %: 31.90% | Epsilon: 0.794\n",
            "Episode 47 | Avg Reward: -31.219 | Oracle Acc: 38.20% | SLA %: 32.20% | Epsilon: 0.790\n",
            "Episode 48 | Avg Reward: -33.492 | Oracle Acc: 36.10% | SLA %: 32.00% | Epsilon: 0.786\n",
            "Episode 49 | Avg Reward: -34.275 | Oracle Acc: 35.80% | SLA %: 33.10% | Epsilon: 0.782\n",
            "Episode 50 | Avg Reward: -33.722 | Oracle Acc: 36.00% | SLA %: 33.00% | Epsilon: 0.778\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from google.colab import files\n",
        "import os\n",
        "\n",
        "CSV_PATH = \"/content/load_balancing_playground.csv\"\n",
        "if not os.path.exists(CSV_PATH):\n",
        "    print(\"Upload load_balancing_playground.csv\")\n",
        "    uploaded = files.upload()\n",
        "    os.rename(list(uploaded.keys())[0], CSV_PATH)\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import random\n",
        "from collections import deque\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "class LoadBalancingEnv:\n",
        "    def __init__(self, csv_path, num_cpus=4, episode_length=1200):\n",
        "        self.df = pd.read_csv(csv_path)\n",
        "        self.num_cpus = num_cpus\n",
        "        self.episode_length = episode_length\n",
        "\n",
        "    def reset(self):\n",
        "        self.start = np.random.randint(0, len(self.df) - self.episode_length)\n",
        "        self.step_id = 0\n",
        "        return self._state()\n",
        "\n",
        "    def step(self, action):\n",
        "        row = self.df.iloc[self.start + self.step_id]\n",
        "\n",
        "        completion = row[f\"cpu_{action}_wait_time\"] + row[\"task_size\"] / row[f\"cpu_{action}_speed\"]\n",
        "\n",
        "        reward = (\n",
        "            -completion\n",
        "            - 0.2 * row[\"load_variance\"]\n",
        "            - 12.0 * row[\"sla_violation\"]\n",
        "        )\n",
        "\n",
        "        self.step_id += 1\n",
        "        done = self.step_id >= self.episode_length\n",
        "        next_state = None if done else self._state()\n",
        "\n",
        "        info = {\n",
        "            \"oracle_cpu\": row[\"chosen_cpu_oracle\"],\n",
        "            \"sla_violation\": row[\"sla_violation\"]\n",
        "        }\n",
        "\n",
        "        return next_state, reward, done, info\n",
        "\n",
        "    def _state(self):\n",
        "        r = self.df.iloc[self.start + self.step_id]\n",
        "        s = [r[\"task_size\"], r[\"priority\"], r[\"system_load_avg\"], r[\"load_variance\"]]\n",
        "        for i in range(self.num_cpus):\n",
        "            s.extend([\n",
        "                r[f\"cpu_{i}_queue_len\"],\n",
        "                r[f\"cpu_{i}_wait_time\"],\n",
        "                r[f\"cpu_{i}_speed\"]\n",
        "            ])\n",
        "        return np.array(s, dtype=np.float32)\n",
        "\n",
        "    def action_space(self):\n",
        "        return self.num_cpus\n",
        "\n",
        "    def state_dim(self):\n",
        "        return len(self._state())\n",
        "\n",
        "\n",
        "class DQN(nn.Module):\n",
        "    def __init__(self, s_dim, a_dim):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(s_dim, 96),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(96, 96),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(96, a_dim)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class ReplayBuffer:\n",
        "    def __init__(self, s_dim, cap=30000):\n",
        "        self.buf = deque(maxlen=cap)\n",
        "        self.s_dim = s_dim\n",
        "\n",
        "    def push(self, s, a, r, ns, d):\n",
        "        if ns is None:\n",
        "            ns = np.zeros(self.s_dim, dtype=np.float32)\n",
        "        self.buf.append((s, a, r, ns, d))\n",
        "\n",
        "    def sample(self, n):\n",
        "        b = random.sample(self.buf, n)\n",
        "        s,a,r,ns,d = zip(*b)\n",
        "        return (\n",
        "            torch.tensor(s).float().to(device),\n",
        "            torch.tensor(a).long().to(device),\n",
        "            torch.tensor(r).float().to(device),\n",
        "            torch.tensor(ns).float().to(device),\n",
        "            torch.tensor(d).float().to(device)\n",
        "        )\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buf)\n",
        "\n",
        "\n",
        "env = LoadBalancingEnv(CSV_PATH)\n",
        "_ = env.reset()                 \n",
        "sd, ad = env.state_dim(), env.action_space()\n",
        "\n",
        "policy = DQN(sd, ad).to(device)\n",
        "target = DQN(sd, ad).to(device)\n",
        "target.load_state_dict(policy.state_dict())\n",
        "\n",
        "opt = optim.Adam(policy.parameters(), lr=1e-3)\n",
        "buf = ReplayBuffer(sd)\n",
        "\n",
        "gamma = 0.99\n",
        "eps, eps_min, eps_decay = 1.0, 0.05, 0.97\n",
        "batch = 64\n",
        "episodes = 60\n",
        "target_update = 100\n",
        "\n",
        "for ep in range(episodes):\n",
        "    s = env.reset()\n",
        "    R, sla, oracle = 0, 0, 0\n",
        "\n",
        "    for t in range(env.episode_length):\n",
        "        if random.random() < eps:\n",
        "            a = random.randint(0, ad-1)\n",
        "        else:\n",
        "            with torch.no_grad():\n",
        "                a = policy(torch.tensor(s).to(device)).argmax().item()\n",
        "\n",
        "        ns, r, d, info = env.step(a)\n",
        "        buf.push(s, a, r, ns, d)\n",
        "\n",
        "        R += r\n",
        "        sla += info[\"sla_violation\"]\n",
        "        oracle += int(a == info[\"oracle_cpu\"])\n",
        "        s = ns\n",
        "\n",
        "        if len(buf) > batch:\n",
        "            S,A,Rw,NS,D = buf.sample(batch)\n",
        "            q = policy(S).gather(1, A.unsqueeze(1)).squeeze()\n",
        "            tq = target(NS).max(1)[0]\n",
        "            y = Rw + gamma * tq * (1-D)\n",
        "            loss = nn.MSELoss()(q, y.detach())\n",
        "            opt.zero_grad()\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "\n",
        "        if t % target_update == 0:\n",
        "            target.load_state_dict(policy.state_dict())\n",
        "\n",
        "        if d:\n",
        "            break\n",
        "\n",
        "    eps = max(eps*eps_decay, eps_min)\n",
        "\n",
        "    print(\n",
        "        f\"Ep {ep+1:02d} | \"\n",
        "        f\"Reward: {R/env.episode_length:.2f} | \"\n",
        "        f\"Oracle: {oracle/env.episode_length*100:.1f}% | \"\n",
        "        f\"SLA: {sla/env.episode_length*100:.1f}% | \"\n",
        "        f\"Eps: {eps:.2f}\"\n",
        "    )\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lbmjoczPEqTV",
        "outputId": "ade62fdf-0f8d-4fea-f1d0-ad2cc0e39f2e"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ep 01 | Reward: -41.55 | Oracle: 25.8% | SLA: 34.1% | Eps: 0.97\n",
            "Ep 02 | Reward: -41.21 | Oracle: 26.1% | SLA: 34.1% | Eps: 0.94\n",
            "Ep 03 | Reward: -39.28 | Oracle: 25.3% | SLA: 31.4% | Eps: 0.91\n",
            "Ep 04 | Reward: -39.41 | Oracle: 30.1% | SLA: 33.2% | Eps: 0.89\n",
            "Ep 05 | Reward: -38.50 | Oracle: 31.5% | SLA: 33.8% | Eps: 0.86\n",
            "Ep 06 | Reward: -38.95 | Oracle: 33.5% | SLA: 33.9% | Eps: 0.83\n",
            "Ep 07 | Reward: -37.50 | Oracle: 35.8% | SLA: 33.7% | Eps: 0.81\n",
            "Ep 08 | Reward: -39.21 | Oracle: 33.5% | SLA: 33.4% | Eps: 0.78\n",
            "Ep 09 | Reward: -36.44 | Oracle: 36.2% | SLA: 33.5% | Eps: 0.76\n",
            "Ep 10 | Reward: -36.84 | Oracle: 36.9% | SLA: 34.8% | Eps: 0.74\n",
            "Ep 11 | Reward: -35.53 | Oracle: 39.6% | SLA: 32.8% | Eps: 0.72\n",
            "Ep 12 | Reward: -34.66 | Oracle: 41.2% | SLA: 33.1% | Eps: 0.69\n",
            "Ep 13 | Reward: -33.88 | Oracle: 41.8% | SLA: 32.8% | Eps: 0.67\n",
            "Ep 14 | Reward: -33.44 | Oracle: 41.5% | SLA: 31.8% | Eps: 0.65\n",
            "Ep 15 | Reward: -33.40 | Oracle: 44.8% | SLA: 33.4% | Eps: 0.63\n",
            "Ep 16 | Reward: -33.26 | Oracle: 44.8% | SLA: 34.9% | Eps: 0.61\n",
            "Ep 17 | Reward: -30.98 | Oracle: 46.9% | SLA: 30.8% | Eps: 0.60\n",
            "Ep 18 | Reward: -32.49 | Oracle: 48.5% | SLA: 34.5% | Eps: 0.58\n",
            "Ep 19 | Reward: -31.79 | Oracle: 49.5% | SLA: 35.0% | Eps: 0.56\n",
            "Ep 20 | Reward: -32.84 | Oracle: 47.8% | SLA: 34.6% | Eps: 0.54\n",
            "Ep 21 | Reward: -32.86 | Oracle: 49.9% | SLA: 33.6% | Eps: 0.53\n",
            "Ep 22 | Reward: -31.69 | Oracle: 47.9% | SLA: 32.3% | Eps: 0.51\n",
            "Ep 23 | Reward: -31.50 | Oracle: 53.2% | SLA: 32.8% | Eps: 0.50\n",
            "Ep 24 | Reward: -29.89 | Oracle: 53.2% | SLA: 32.8% | Eps: 0.48\n",
            "Ep 25 | Reward: -29.77 | Oracle: 53.4% | SLA: 32.2% | Eps: 0.47\n",
            "Ep 26 | Reward: -28.45 | Oracle: 54.0% | SLA: 31.3% | Eps: 0.45\n",
            "Ep 27 | Reward: -28.46 | Oracle: 54.9% | SLA: 33.3% | Eps: 0.44\n",
            "Ep 28 | Reward: -28.87 | Oracle: 56.6% | SLA: 34.2% | Eps: 0.43\n",
            "Ep 29 | Reward: -28.26 | Oracle: 54.9% | SLA: 30.6% | Eps: 0.41\n",
            "Ep 30 | Reward: -28.03 | Oracle: 57.5% | SLA: 33.1% | Eps: 0.40\n",
            "Ep 31 | Reward: -27.56 | Oracle: 58.0% | SLA: 31.6% | Eps: 0.39\n",
            "Ep 32 | Reward: -28.00 | Oracle: 61.2% | SLA: 32.6% | Eps: 0.38\n",
            "Ep 33 | Reward: -26.39 | Oracle: 61.1% | SLA: 32.4% | Eps: 0.37\n",
            "Ep 34 | Reward: -28.03 | Oracle: 60.1% | SLA: 32.5% | Eps: 0.36\n",
            "Ep 35 | Reward: -26.19 | Oracle: 61.0% | SLA: 31.5% | Eps: 0.34\n",
            "Ep 36 | Reward: -28.60 | Oracle: 62.9% | SLA: 34.3% | Eps: 0.33\n",
            "Ep 37 | Reward: -25.82 | Oracle: 61.5% | SLA: 32.0% | Eps: 0.32\n",
            "Ep 38 | Reward: -26.46 | Oracle: 60.4% | SLA: 35.2% | Eps: 0.31\n",
            "Ep 39 | Reward: -25.57 | Oracle: 61.9% | SLA: 33.4% | Eps: 0.30\n",
            "Ep 40 | Reward: -25.69 | Oracle: 66.2% | SLA: 32.2% | Eps: 0.30\n",
            "Ep 41 | Reward: -26.02 | Oracle: 64.6% | SLA: 33.9% | Eps: 0.29\n",
            "Ep 42 | Reward: -25.69 | Oracle: 64.3% | SLA: 32.2% | Eps: 0.28\n",
            "Ep 43 | Reward: -25.20 | Oracle: 68.5% | SLA: 34.4% | Eps: 0.27\n",
            "Ep 44 | Reward: -25.29 | Oracle: 64.2% | SLA: 33.4% | Eps: 0.26\n",
            "Ep 45 | Reward: -25.69 | Oracle: 65.8% | SLA: 36.7% | Eps: 0.25\n",
            "Ep 46 | Reward: -25.53 | Oracle: 65.6% | SLA: 33.1% | Eps: 0.25\n",
            "Ep 47 | Reward: -25.69 | Oracle: 64.9% | SLA: 34.6% | Eps: 0.24\n",
            "Ep 48 | Reward: -25.56 | Oracle: 65.7% | SLA: 34.0% | Eps: 0.23\n",
            "Ep 49 | Reward: -24.40 | Oracle: 67.6% | SLA: 32.6% | Eps: 0.22\n",
            "Ep 50 | Reward: -24.88 | Oracle: 66.8% | SLA: 33.8% | Eps: 0.22\n",
            "Ep 51 | Reward: -22.94 | Oracle: 67.9% | SLA: 31.2% | Eps: 0.21\n",
            "Ep 52 | Reward: -25.01 | Oracle: 66.1% | SLA: 33.3% | Eps: 0.21\n",
            "Ep 53 | Reward: -24.80 | Oracle: 65.3% | SLA: 33.6% | Eps: 0.20\n",
            "Ep 54 | Reward: -24.50 | Oracle: 65.3% | SLA: 34.1% | Eps: 0.19\n",
            "Ep 55 | Reward: -22.23 | Oracle: 69.2% | SLA: 31.8% | Eps: 0.19\n",
            "Ep 56 | Reward: -22.89 | Oracle: 67.1% | SLA: 31.4% | Eps: 0.18\n",
            "Ep 57 | Reward: -23.48 | Oracle: 68.6% | SLA: 34.8% | Eps: 0.18\n",
            "Ep 58 | Reward: -22.45 | Oracle: 67.4% | SLA: 30.9% | Eps: 0.17\n",
            "Ep 59 | Reward: -22.97 | Oracle: 68.6% | SLA: 34.7% | Eps: 0.17\n",
            "Ep 60 | Reward: -23.10 | Oracle: 69.8% | SLA: 33.1% | Eps: 0.16\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome To Colab",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
